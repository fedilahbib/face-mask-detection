{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "FaceMask-Detection.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDPOUg3azl1s"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "from os import getcwd\n",
        "from os import listdir\n",
        "import cv2\n",
        "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils import shuffle\n",
        "import imutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image  as mpimg"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpryeT5UzqEc"
      },
      "source": [
        "/content/drive/MyDrive/data/data/with_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93bDzRqezq3Q",
        "outputId": "8908f103-203a-4f75-f471-5aabf1c7872c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFaCNQMhzl1z",
        "outputId": "a278fd1e-ec9e-4687-f31c-8ab643522651"
      },
      "source": [
        "print(\"The number of images with facemask labelled 'yes':\",len(os.listdir('/content/drive/MyDrive/data/data/with_mask')))\n",
        "print(\"The number of images with facemask labelled 'no':\",len(os.listdir('/content/drive/MyDrive/data/data/without_mask')))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of images with facemask labelled 'yes': 690\n",
            "The number of images with facemask labelled 'no': 686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmY1LgXmzl11",
        "outputId": "9e3661c3-68fe-40a3-98cf-2fe70f48a1f3"
      },
      "source": [
        "def data_summary(main_path):\n",
        "    \n",
        "    yes_path = main_path+'with_mask'\n",
        "    no_path = main_path+'without_mask'\n",
        "        \n",
        "    # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n",
        "    m_pos = len(listdir(yes_path))\n",
        "    # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n",
        "    m_neg = len(listdir(no_path))\n",
        "    # number of all examples\n",
        "    m = (m_pos+m_neg)\n",
        "    \n",
        "    pos_prec = (m_pos* 100.0)/ m\n",
        "    neg_prec = (m_neg* 100.0)/ m\n",
        "    \n",
        "    print(f\"Number of examples: {m}\")\n",
        "    print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\") \n",
        "    print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\") \n",
        "    \n",
        "augmented_data_path = '/content/drive/MyDrive/data/data/'    \n",
        "data_summary(augmented_data_path)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of examples: 1376\n",
            "Percentage of positive examples: 50.145348837209305%, number of pos examples: 690\n",
            "Percentage of negative examples: 49.854651162790695%, number of neg examples: 686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b73PVwEuzl12"
      },
      "source": [
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    dataset = []\n",
        "    \n",
        "    for unitData in os.listdir(SOURCE):\n",
        "        data = SOURCE + unitData\n",
        "        if(os.path.getsize(data) > 0):\n",
        "            dataset.append(unitData)\n",
        "        else:\n",
        "            print('Skipped ' + unitData)\n",
        "            print('Invalid file i.e zero size')\n",
        "    \n",
        "    train_set_length = int(len(dataset) * SPLIT_SIZE)\n",
        "    test_set_length = int(len(dataset) - train_set_length)\n",
        "    shuffled_set = random.sample(dataset, len(dataset))\n",
        "    train_set = dataset[0:train_set_length]\n",
        "    test_set = dataset[-test_set_length:]\n",
        "       \n",
        "    for unitData in train_set:\n",
        "        temp_train_set = SOURCE + unitData\n",
        "        final_train_set = TRAINING + unitData\n",
        "        copyfile(temp_train_set, final_train_set)\n",
        "    \n",
        "    for unitData in test_set:\n",
        "        temp_test_set = SOURCE + unitData\n",
        "        final_test_set = TESTING + unitData\n",
        "        copyfile(temp_test_set, final_test_set)\n",
        "        \n",
        "        \n",
        "YES_SOURCE_DIR = \"/content/drive/MyDrive/data/data/augmented data1/val/with_mask/\"\n",
        "TRAINING_YES_DIR = \"/content/drive/MyDrive/data/data/augmented data1/train/with_mask/\"\n",
        "TESTING_YES_DIR = \"/content/drive/MyDrive/data/data/augmented data1/test/with_mask/\"\n",
        "NO_SOURCE_DIR = \"/content/drive/MyDrive/data/data/augmented data1/val/without_mask/\"\n",
        "TRAINING_NO_DIR = \"/content/drive/MyDrive/data/data/augmented data1/train/without_mask/\"\n",
        "TESTING_NO_DIR = \"/content/drive/MyDrive/data/data/augmented data1/test/without_mask/\"\n",
        "split_size = .8\n",
        "split_data(YES_SOURCE_DIR, TRAINING_YES_DIR, TESTING_YES_DIR, split_size)\n",
        "split_data(NO_SOURCE_DIR, TRAINING_NO_DIR, TESTING_NO_DIR, split_size)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEpNDM2yzl14",
        "outputId": "70791bd3-9392-43a3-f962-7b13e7001436"
      },
      "source": [
        "print(\"The number of images with facemask in the training set labelled 'yes':\", len(os.listdir('/content/drive/MyDrive/data/data/augmented data1/train/with_mask/')))\n",
        "print(\"The number of images with facemask in the test set labelled 'yes':\", len(os.listdir('/content/drive/MyDrive/data/data/augmented data1/test/with_mask/')))\n",
        "print(\"The number of images without facemask in the training set labelled 'no':\", len(os.listdir('/content/drive/MyDrive/data/data/augmented data1/train/without_mask/')))\n",
        "print(\"The number of images without facemask in the test set labelled 'no':\", len(os.listdir('/content/drive/MyDrive/data/data/augmented data1/test/without_mask/')))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of images with facemask in the training set labelled 'yes': 56\n",
            "The number of images with facemask in the test set labelled 'yes': 112\n",
            "The number of images without facemask in the training set labelled 'no': 216\n",
            "The number of images without facemask in the test set labelled 'no': 112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRE_2b9bzl15"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(100, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(50, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFkX9IEIzl16",
        "outputId": "74bd2bcf-ed6b-40c7-eaea-b4b26988819c"
      },
      "source": [
        "TRAINING_DIR = \"/content/drive/MyDrive/data/data/augmented data1/train/\"\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
        "                                                    batch_size=10, \n",
        "                                                    target_size=(150, 150))\n",
        "VALIDATION_DIR = \"/content/drive/MyDrive/data/data/augmented data1/test/\"\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
        "                                                         batch_size=10, \n",
        "                                                         target_size=(150, 150))\n",
        "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 359 images belonging to 2 classes.\n",
            "Found 224 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2kw0wLMzl17",
        "outputId": "98ecb30f-ce65-4c8b-ae57-6cf81beb352c"
      },
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              epochs=30,\n",
        "                              validation_data=validation_generator,\n",
        "                              callbacks=[checkpoint])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "36/36 [==============================] - 170s 5s/step - loss: 0.5404 - acc: 0.8384 - val_loss: 0.8037 - val_acc: 0.5179\n",
            "INFO:tensorflow:Assets written to: model-001.model/assets\n",
            "Epoch 2/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.3932 - acc: 0.8440 - val_loss: 0.7530 - val_acc: 0.5134\n",
            "INFO:tensorflow:Assets written to: model-002.model/assets\n",
            "Epoch 3/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.2750 - acc: 0.8802 - val_loss: 0.3418 - val_acc: 0.8705\n",
            "INFO:tensorflow:Assets written to: model-003.model/assets\n",
            "Epoch 4/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.2596 - acc: 0.9025 - val_loss: 0.3338 - val_acc: 0.8795\n",
            "INFO:tensorflow:Assets written to: model-004.model/assets\n",
            "Epoch 5/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.3746 - acc: 0.8412 - val_loss: 0.3469 - val_acc: 0.8795\n",
            "Epoch 6/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.2569 - acc: 0.8969 - val_loss: 0.2219 - val_acc: 0.9420\n",
            "INFO:tensorflow:Assets written to: model-006.model/assets\n",
            "Epoch 7/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.2126 - acc: 0.9220 - val_loss: 0.3327 - val_acc: 0.8661\n",
            "Epoch 8/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.2069 - acc: 0.9248 - val_loss: 0.4897 - val_acc: 0.7991\n",
            "Epoch 9/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.2518 - acc: 0.9081 - val_loss: 0.1363 - val_acc: 0.9554\n",
            "INFO:tensorflow:Assets written to: model-009.model/assets\n",
            "Epoch 10/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.1928 - acc: 0.9331 - val_loss: 0.1544 - val_acc: 0.9643\n",
            "Epoch 11/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.1218 - acc: 0.9471 - val_loss: 0.1141 - val_acc: 0.9643\n",
            "INFO:tensorflow:Assets written to: model-011.model/assets\n",
            "Epoch 12/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.1703 - acc: 0.9276 - val_loss: 0.0811 - val_acc: 0.9777\n",
            "INFO:tensorflow:Assets written to: model-012.model/assets\n",
            "Epoch 13/30\n",
            "36/36 [==============================] - 41s 1s/step - loss: 0.1709 - acc: 0.9387 - val_loss: 0.0933 - val_acc: 0.9821\n",
            "Epoch 14/30\n",
            "36/36 [==============================] - 39s 1s/step - loss: 0.1721 - acc: 0.9304 - val_loss: 0.1262 - val_acc: 0.9821\n",
            "Epoch 15/30\n",
            "36/36 [==============================] - 39s 1s/step - loss: 0.1532 - acc: 0.9331 - val_loss: 0.1076 - val_acc: 0.9732\n",
            "Epoch 16/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.2602 - acc: 0.8858 - val_loss: 0.2440 - val_acc: 0.8973\n",
            "Epoch 17/30\n",
            "36/36 [==============================] - 39s 1s/step - loss: 0.2154 - acc: 0.9025 - val_loss: 0.1653 - val_acc: 0.9375\n",
            "Epoch 18/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.1729 - acc: 0.9304 - val_loss: 0.1868 - val_acc: 0.9464\n",
            "Epoch 19/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.1286 - acc: 0.9471 - val_loss: 0.1347 - val_acc: 0.9330\n",
            "Epoch 20/30\n",
            "36/36 [==============================] - 39s 1s/step - loss: 0.1832 - acc: 0.9359 - val_loss: 0.1549 - val_acc: 0.9152\n",
            "Epoch 21/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.1359 - acc: 0.9499 - val_loss: 0.0952 - val_acc: 0.9821\n",
            "Epoch 22/30\n",
            "36/36 [==============================] - 39s 1s/step - loss: 0.1136 - acc: 0.9666 - val_loss: 0.0886 - val_acc: 0.9732\n",
            "Epoch 23/30\n",
            "36/36 [==============================] - 39s 1s/step - loss: 0.0995 - acc: 0.9554 - val_loss: 0.1780 - val_acc: 0.9554\n",
            "Epoch 24/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.2120 - acc: 0.9109 - val_loss: 0.1297 - val_acc: 0.9643\n",
            "Epoch 25/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.1175 - acc: 0.9582 - val_loss: 0.0857 - val_acc: 0.9777\n",
            "Epoch 26/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.1281 - acc: 0.9499 - val_loss: 0.0511 - val_acc: 0.9777\n",
            "INFO:tensorflow:Assets written to: model-026.model/assets\n",
            "Epoch 27/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.1087 - acc: 0.9638 - val_loss: 0.0751 - val_acc: 0.9732\n",
            "Epoch 28/30\n",
            "36/36 [==============================] - 40s 1s/step - loss: 0.1120 - acc: 0.9526 - val_loss: 0.0823 - val_acc: 0.9777\n",
            "Epoch 29/30\n",
            "36/36 [==============================] - 39s 1s/step - loss: 0.0874 - acc: 0.9610 - val_loss: 0.0959 - val_acc: 0.9732\n",
            "Epoch 30/30\n",
            "36/36 [==============================] - 38s 1s/step - loss: 0.1192 - acc: 0.9638 - val_loss: 0.1494 - val_acc: 0.9464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCvJIn8izl18"
      },
      "source": [
        "face_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "ykzonPmTzl18",
        "outputId": "3e9d2ae8-8f3a-41c0-b7bc-bcc6c124ef81"
      },
      "source": [
        "labels_dict={0:'without_mask',1:'with_mask'}\n",
        "color_dict={0:(0,0,255),1:(0,255,0)}\n",
        "\n",
        "size = 4\n",
        "webcam = cv2.VideoCapture(0) #Use camera 0\n",
        "\n",
        "# We load the xml file\n",
        "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "while True:\n",
        "    (rval, im) = webcam.read()\n",
        "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
        "\n",
        "    # Resize the image to speed up detection\n",
        "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
        "\n",
        "    # detect MultiScale / faces \n",
        "    faces = classifier.detectMultiScale(mini)\n",
        "\n",
        "    # Draw rectangles around each face\n",
        "    for f in faces:\n",
        "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
        "        #Save just the rectangle faces in SubRecFaces\n",
        "        face_img = im[y:y+h, x:x+w]\n",
        "        resized=cv2.resize(face_img,(150,150))\n",
        "        normalized=resized/255.0\n",
        "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
        "        reshaped = np.vstack([reshaped])\n",
        "        result=model.predict(reshaped)\n",
        "        #print(result)\n",
        "        \n",
        "        label=np.argmax(result,axis=1)[0]\n",
        "      \n",
        "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
        "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
        "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
        "        \n",
        "    # Show the image\n",
        "    cv2.imshow('LIVE',   im)\n",
        "    key = cv2.waitKey(10)\n",
        "    # if Esc key is press then break out of the loop \n",
        "    if key == 27: #The Esc key\n",
        "        break\n",
        "# Stop video\n",
        "webcam.release()\n",
        "\n",
        "# Close all started windows\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-86a0855a7334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Resize the image to speed up detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# detect MultiScale / faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ]
}